# Attention-Based-Visual-Odometry

In this project, we are developing a novel artificial neural network model that can be used to calculate visual odometry. The proposed model is a temporal-based attention neural network, the model takes in raw pixel and depth values from a camera and uses these inputs to generate feature vectors. The model then stores a history of the generated feature vectors, which allows it to take into account the temporal relationship between the current and past data points. 
The resulting output of the model is a prediction of the odometry value, which represents the movement of the camera or object. 
The proposed model has been tested in a variety of scenarios and has shown robust performance in unknown and cluttered environments.

## ğŸ’½ Dataset
### 7Scenes
The 7-Scenes dataset is a collection of tracked RGB-D camera frames.
To download the entire dataset follow [OFFICIAL LINK](https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/).

 ![Alt text](assets/7-scenes-7-scenes-overview.png)
 :--:
  *7Scenes* 
[Download](https://drive.google.com/drive/folders/1v_y4HpT9IifWIbxeZtm9XKjBth6tM5mr?usp=share_link) 7Scenes dataset having "heads" sequence

### NYU sparse dataset
Similarly, NYU sparse dataset is a collection of tracked RGB-D camera frames in featureless environment of 5 Metrotech.
![Alt text](assets/Floorplan.png)
 :--:
  
 ![Alt text](assets/NYC-sparse-dataset.png)
 :--:
  *NYU Sparse Dataset*
[Download](https://drive.google.com/drive/folders/1zT49fvm58KhV8maMLzybn7Jrsi49X3Je?usp=share_link) NYU sparse dataset
[Download](https://drive.google.com/uc?export=download&id=1H_JrvroG4v_MMUqWKM0P57lNYYE2droc) NYU sparse dataset(ROS bag file)


## âš™ï¸ Setup

1. Install all the requirements (PyTorch v1.13.0)
```shell
pip3 install torch torchvision torchaudio torchinfo tensorboard matplotlib numpy pandas opencv-python 
```
2. Clone the GitHub repository
```shell
git clone https://github.com/navoday01/Attention-Based-Visual-Odometry.git
```
3. [Download](https://drive.google.com/uc?export=download&id=1dhdH5g1XCGnFbvOj5OOUY3YHaCO4RIIf) Model and paste inside Attention-Based-Visual-Odometry folder.

## ğŸ“‰ Tensorboard
To view the losses use Tensorboard to visualize the graphs
```shell
tensorboard --logdir runs/
```

## â³ Training
Run train script `training.py` to recreate similar model.
```shell
python3 training.py <Path to 7Scenes dataset folder> <Scene for Training>
```
example: python3 training.py /home/7scenes heads
## ğŸ–¼ Testing

 To test the model, run `test.py` and the predicited camera poses along with the actual camera poses get saved as a .csv file. The .csv file can be used to vizualize the camera trajectory.
```shell
python3 test.py <Path to 7Scenes dataset folder> <Scene for Testing>
```
example: python3 test.py /home/7scenes heads

To test the model for the NYU sparse dataset
```shell
python3 test.py <Path to 7Scenes dataset folder> 5metrotech
```


## ğŸ“Š Results

 ![Alt text](assets/Plots.png)
 :--:
  *Losses* 

![Alt text](assets/Trajectory.png)
 :--:
  *Trajectory* 

## ğŸ“¶ Benchmarks
| **Scene** |   **SCoRe Forest**    |  **DSAC**  |  **VLocNet++**  |  **NNnet**  |  **PoseNet2**  |  **Ours**  |
|:--------:|:------------------:|:----------------------------:|:------------:|:----------:|:-------------:|:-------------:|
|  Chess |  0.03m, **0.66Â°**     |     0.03m, **0.66Â°**       |    **0.0018m**, 1.17Â°    |   0.13m, 6.66Â°  |  0.13m, 4.48Â°    | 0.01m, 3.35Â° |
|  Fire  |  0.05m, 1.50Â° |     0.04m, 1.50Â°       |    **0.009m**, **0.61Â°**  |   0.26m, 12.72Â°  |  0.27m, 11.28Â°    | 0.013m, 2.55Â°    |
|  Heads |  0.06m, 5.50Â°    |     0.03m, 2.70Â°       |    **0.008m**, **0.60Â°**     |  0.14m, 12.34Â°  |  0.17m, 13.00Â°    |  0.009m, 1.03Â°         |
| Office |  0.04m, 0.78Â°    |     0.04m, 1.60Â°    |    0.016m, 0.78Â°     |   0.21m, 7.35Â°  |  0.19m, 5.35Â°     | **0.014m**, **0.77Â°**           |
| Pumpkin|  0.04m, **0.68Â°**   |     0.05m, 2.00Â°   |    0.009m, 0.82Â°   |   0.24m, 6.35Â°  |  0.26m, 4.75Â°     | **0.008m**, 0.75Â°           |
|RedKitchen|  0.04m, **0.76Â°**      |     0.05m, 2.00Â°    |    **0.0017m**, 0.93Â°   |   0.24m, 8.03Â°  |  0.23m, 5.35Â°    |  0.21m, 4.34Â°        |
|  Stairs  |  0.32m, 1.32Â°      |     1.17m, 33.1Â°    |   **0.010m**, **0.48Â°**   |   0.27m, 11.28Â°  |  0.35m, 12.4Â°    |  0.011m, 2.89Â°        |

## ğŸ“¦ Conclusion

A temporal attention (TA) network was created to calculate the visual odometry of the most recent frame using a set of previous frames.

A new dataset called the NYU sparse dataset was created for testing the performance of a visual odometry algorithm in sparse feature environments. The network being tested showed promising results on this dataset and a separate dataset as well. The model's robust performance can be attributed to the fact that the Depth Maps are not being computed but metric Depth Map are taken directly from the dataset, doing this eliminates the scale requirement as it is already encoded in the Depth Map. 

The intent of this project was to create a TA network which was accomplished, further training and testing was conducted on a dataset used in VO benchmarks. Our other goal was to test the model against other state of the art algorithms, to see where it stands. Additionally, the last deliverable was to create a sparse featureless dataset which was also accomplished successfully.

## ğŸ‘©â€âš–ï¸ Acknowledgement

We express our gratitude to the all the authors, who have worked hard to engineer such effective solutions for visual odometry. We would like to thank everyone whose comments and suggestions helped us with the project. We express our sincere gratitude to Professors Chinmay Hegde, Arsalan Mosenia, and the teaching assistant Teal Witter. 
